{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bd9692-1164-4eec-9344-837b22a8924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/torchvision-0.21.0+7af6987-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/setuptools-75.8.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/pillow-11.1.0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/torchvision-0.21.0+7af6987-py3.12-linux-x86_64.egg (0.21.0+7af6987)\n",
      "Requirement already satisfied: filelock in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/setuptools-75.8.0-py3.12.egg (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/pillow-11.1.0-py3.12-linux-x86_64.egg (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/torchvision-0.21.0+7af6987-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/setuptools-75.8.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/pillow-11.1.0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: adabelief-pytorch in /global/homes/n/natekv/.local/perlmutter/pytorch2.6.0/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: torch>=0.4.0 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from adabelief-pytorch) (2.6.0)\n",
      "Requirement already satisfied: colorama>=0.4.0 in /global/homes/n/natekv/.local/perlmutter/pytorch2.6.0/lib/python3.12/site-packages (from adabelief-pytorch) (0.4.6)\n",
      "Requirement already satisfied: tabulate>=0.7 in /global/homes/n/natekv/.local/perlmutter/pytorch2.6.0/lib/python3.12/site-packages (from adabelief-pytorch) (0.9.0)\n",
      "Requirement already satisfied: filelock in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch>=0.4.0->adabelief-pytorch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/setuptools-75.8.0-py3.12.egg (from torch>=0.4.0->adabelief-pytorch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch>=0.4.0->adabelief-pytorch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from torch>=0.4.0->adabelief-pytorch) (2024.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from sympy==1.13.1->torch>=0.4.0->adabelief-pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages (from jinja2->torch>=0.4.0->adabelief-pytorch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install adabelief-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b5c8430-b923-4a9c-bcb9-cd54c1828a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6575f2-321f-4339-a999-87fd93c10147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 21 08:21:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:03:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             70W /  400W |    1579MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             72W /  400W |    1529MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:82:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             72W /  400W |    1529MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             72W /  400W |    1529MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1945440      C   python                                       1568MiB |\n",
      "|    1   N/A  N/A   1945440      C   python                                       1518MiB |\n",
      "|    2   N/A  N/A   1945440      C   python                                       1518MiB |\n",
      "|    3   N/A  N/A   1945440      C   python                                       1518MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627487ae-cce8-4858-a536-6b3b50d3f55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain: (10000, 125, 125, 1) ytrain: (10000, 1) ttrain: (10000, 1)\n",
      "xval:   (20000, 125, 125, 1) yval:   (20000, 1) tval:   (20000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "path = \"/global/cfs/projectdirs/deepsrch/jwst_sims/pristine_bright/\"\n",
    "\n",
    "# Load data\n",
    "x0 = np.load(os.path.join(path, \"images.npy\"))        # (N,H,W)\n",
    "y0 = np.load(os.path.join(path, \"lensed.npy\"))        # (N,)\n",
    "theta_e = np.load(os.path.join(path, \"theta_Es.npy\")) # (N,)\n",
    "\n",
    "# Types\n",
    "x0 = x0.astype(np.float32)\n",
    "y0 = y0.astype(np.int64)\n",
    "theta_e = theta_e.astype(np.float32)\n",
    "\n",
    "# Normalize images (global)\n",
    "mean = x0.mean()\n",
    "std  = x0.std() + 1e-8\n",
    "x0 = (x0 - mean) / std\n",
    "\n",
    "# === Hard split ===\n",
    "non_lensed_mask = (y0 == 0)\n",
    "lensed_mask     = (y0 == 1)\n",
    "\n",
    "xtrain = x0[non_lensed_mask]\n",
    "ytrain = y0[non_lensed_mask]\n",
    "ttrain = theta_e[non_lensed_mask]   # <-- add this\n",
    "\n",
    "xval = x0\n",
    "yval = y0\n",
    "tval = theta_e                      # <-- add this\n",
    "\n",
    "# Reshape to channels-last for your VQ-VAE code\n",
    "xtrain = xtrain[..., None]          # (N,125,125,1)\n",
    "xval   = xval[..., None]\n",
    "\n",
    "# Keep labels as (N,1) if your code expects that\n",
    "ytrain = ytrain.reshape(-1, 1)\n",
    "yval   = yval.reshape(-1, 1)\n",
    "\n",
    "# Theta: you can keep (N,) or reshape to (N,1) — choose one and stay consistent\n",
    "ttrain = ttrain.reshape(-1, 1)\n",
    "tval   = tval.reshape(-1, 1)\n",
    "\n",
    "print(\"xtrain:\", xtrain.shape, \"ytrain:\", ytrain.shape, \"ttrain:\", ttrain.shape)\n",
    "print(\"xval:  \", xval.shape,   \"yval:  \", yval.shape,   \"tval:  \", tval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02dd888e-0ca9-4f56-afa5-cbfc4690c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_channels=64, z_channels=128):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)  # 125 → 63\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, stride=1, padding=1)  # stays 63\n",
    "        self.conv3 = nn.Conv2d(hidden_channels, z_channels, kernel_size=3, stride=1, padding=1)  # stays 63\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.relu(self.conv1(x))\n",
    "        out2 = self.relu(self.conv2(out1))\n",
    "        z = self.conv3(out2)\n",
    "\n",
    "        return z, out1 \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_channels=128, hidden_channels=64, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decode_main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_channels, hidden_channels, kernel_size=4, stride=2, padding=1),  # 63 → 126\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # After upsampling, combine with skip connection from encoder\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels + hidden_channels, hidden_channels, kernel_size=3, stride=1, padding=1),  # Skip concat\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, out_channels, kernel_size=3, stride=1, padding=1)  # Output: (1, 125, 125)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, skip_connection):\n",
    "        x = self.decode_main(z)  # Upsample\n",
    "\n",
    "        # Crop skip connection to match (in case of size mismatch)\n",
    "        skip_upsampled = F.interpolate(skip_connection, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, skip_upsampled], dim=1)\n",
    "        x = self.final_layers(x)\n",
    "\n",
    "        return x[:, :, :125, :125] \n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings=64, embedding_dim=32, commitment_cost=0.50):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: [B, C, H, W] → [B, H, W, C]\n",
    "        z_perm = z.permute(0, 2, 3, 1).contiguous()\n",
    "        z_flattened = z_perm.view(-1, self.embedding_dim)\n",
    "    \n",
    "        # Compute distances\n",
    "        distances = (\n",
    "            torch.sum(z_flattened**2, dim=1, keepdim=True)\n",
    "            - 2 * torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "            + torch.sum(self.embedding.weight**2, dim=1)\n",
    "        )\n",
    "    \n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.size(0), self.num_embeddings, device=z.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "    \n",
    "        quantized = torch.matmul(encodings, self.embedding.weight)\n",
    "        quantized = quantized.view(z_perm.shape)\n",
    "    \n",
    "        # Undo permute → back to [B, C, H, W]\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "    \n",
    "        # Losses (quantized and z now match: [B, C, H, W])\n",
    "        commitment_loss = self.commitment_cost * F.mse_loss(quantized.detach(), z)\n",
    "        codebook_loss = F.mse_loss(quantized, z.detach())\n",
    "    \n",
    "        # Straight-through estimator\n",
    "        quantized = z + (quantized - z).detach()\n",
    "    \n",
    "        return quantized, commitment_loss + codebook_loss\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, stride=2, padding=1),  # (1, 125, 125) → (64, ~63, ~63)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 1, 4, stride=1, padding=1),  # → (1, ~61, ~61)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.quantizer = VectorQuantizer()\n",
    "\n",
    "    def forward(self, noisy_input, clean_target):\n",
    "        z, skip = self.encoder(noisy_input)\n",
    "        quantized, vq_loss = self.quantizer(z)\n",
    "\n",
    "        residual = self.decoder(quantized, skip)\n",
    "        # Ensure residual matches input size exactly\n",
    "        residual_upsampled = F.interpolate(residual, size=noisy_input.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        x_recon = noisy_input + residual_upsampled\n",
    "        recon_loss = F.l1_loss(x_recon, clean_target)\n",
    "\n",
    "        return x_recon, recon_loss + vq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66ba891-e1b1-4ddc-9c38-1c0e26432358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyDataset(Dataset):\n",
    "    def __init__(self, images, noise_std=0.1, add_poisson=True, exp_time_range=(1e2, 1e6), augment=True):\n",
    "        self.images = images.astype(np.float32).transpose(0, 3, 1, 2)  # (N, 1, 125, 125)\n",
    "        self.noise_std = noise_std\n",
    "        self.add_poisson = add_poisson\n",
    "        self.exp_time_range = exp_time_range  # (min_exp, max_exp)\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]  # Clean image\n",
    "\n",
    "        # ----- Augmentation Block -----\n",
    "        if self.augment:\n",
    "            # Random rotation (0, 90, 180, 270)\n",
    "            k = random.choice([0, 1, 2, 3])\n",
    "            img = np.rot90(img, k, axes=(1, 2)).copy()\n",
    "\n",
    "            # Random flip\n",
    "            if random.random() < 0.5:\n",
    "                img = np.flip(img, axis=1).copy()  # Horizontal\n",
    "            if random.random() < 0.5:\n",
    "                img = np.flip(img, axis=2).copy()  # Vertical\n",
    "\n",
    "            # Mild elliptical stretching (affine shear or zoom)\n",
    "            zoom_factor = np.random.uniform(0.6, 1.4)\n",
    "            img = scipy.ndimage.zoom(img, (1, zoom_factor, 1), order=1)  # Stretch vertically\n",
    "            if img.shape[1] > 125:\n",
    "                img = img[:, :125, :]\n",
    "            elif img.shape[1] < 125:\n",
    "                pad = 125 - img.shape[1]\n",
    "                img = np.pad(img, ((0,0),(0,pad),(0,0)), mode='reflect')\n",
    "\n",
    "            # Flux scaling\n",
    "            flux_scale = np.random.uniform(0.8, 1.2)\n",
    "            img = img * flux_scale\n",
    "\n",
    "        # ----- Add Noise -----\n",
    "        noisy_img = img.copy()\n",
    "\n",
    "        if self.add_poisson:\n",
    "            exp_time = 10 ** np.random.uniform(np.log10(self.exp_time_range[0]), np.log10(self.exp_time_range[1]))\n",
    "            sigma = np.sqrt(np.abs(img) / exp_time).astype(np.float32)\n",
    "            poisson_noise = np.random.normal(0, 1, img.shape).astype(np.float32) * sigma\n",
    "            noisy_img += poisson_noise\n",
    "\n",
    "        gaussian_noise = np.random.normal(0, self.noise_std, img.shape).astype(np.float32)\n",
    "        noisy_img += gaussian_noise\n",
    "\n",
    "        return torch.tensor(noisy_img, dtype=torch.float32), torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, images, labels, theta_e=None, arc=None):\n",
    "        self.images = images.astype(np.float32).transpose(0, 3, 1, 2)\n",
    "        self.labels = labels\n",
    "        self.theta_e = theta_e.astype(np.float32) if theta_e is not None else None\n",
    "        self.arc = arc.astype(np.float32).transpose(0, 3, 1, 2) if arc is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.tensor(self.images[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "\n",
    "        out = [image, label]\n",
    "\n",
    "        if self.theta_e is not None:\n",
    "            out.append(torch.tensor(self.theta_e[idx], dtype=torch.float32))  # scalar\n",
    "\n",
    "        if self.arc is not None:\n",
    "            arc = torch.tensor(self.arc[idx], dtype=torch.float32)\n",
    "            out.append(arc)\n",
    "\n",
    "        return tuple(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c673bd-64eb-4e25-a7ad-dc62e6cf068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "\n",
    "epochs = 200\n",
    "lr= 2.5e-4\n",
    "patience = 3\n",
    "model_path = \"models/3_channel_vq_vae_2.pth\"\n",
    "bs = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e6ae8c-3e2e-4213-bf1f-8afd038859d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "path = \"/global/cfs/projectdirs/deepsrch/jwst_sims/pristine_bright/\"\n",
    "\n",
    "# load theta_e for the full dataset (same indexing as xval/yval)\n",
    "theta_e = np.load(os.path.join(path, \"theta_Es.npy\")).astype(np.float32)\n",
    "\n",
    "# Apply nothing to either set\n",
    "train_dataset = GalaxyDataset(xtrain)\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "# IMPORTANT: InferenceDataset must accept theta_e (we'll update the class accordingly)\n",
    "inference_dataset = InferenceDataset(xval, yval, theta_e=theta_e)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=1, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c147ed7-bcf0-4bd6-98d2-16aee382ba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "model = VQVAE().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = AdaBelief(model.parameters(), lr=lr, eps=1e-8, betas=(0.9, 0.999),\n",
    "                        weight_decay=1e-5, rectify=False)\n",
    "optimizer_D = AdaBelief(discriminator.parameters(), lr=lr, eps=1e-8, betas=(0.9, 0.999),\n",
    "                        weight_decay=1e-5, rectify=False)\n",
    "\n",
    "# Loss and GAN weight\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "lambda_adv = 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0777dd9-71de-478a-860c-9dcf3ae146cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Training: 100%|██████████| 1250/1250 [00:29<00:00, 41.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.087708 | Val Loss: 0.005167\n",
      "Saved model (val loss: 0.005167)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Training: 100%|██████████| 1250/1250 [00:29<00:00, 42.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.088857 | Val Loss: 0.005915\n",
      "No val improvement. Patience 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Training: 100%|██████████| 1250/1250 [00:29<00:00, 42.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.089918 | Val Loss: 0.005012\n",
      "Saved model (val loss: 0.005012)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Training: 100%|██████████| 1250/1250 [00:29<00:00, 42.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.090250 | Val Loss: 0.006399\n",
      "No val improvement. Patience 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Training: 100%|██████████| 1250/1250 [00:29<00:00, 42.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.090261 | Val Loss: 0.006406\n",
      "No val improvement. Patience 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Training: 100%|██████████| 1250/1250 [00:29<00:00, 42.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.092404 | Val Loss: 0.008639\n",
      "No val improvement. Patience 3/3\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    discriminator.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for noisy_batch, clean_batch in tqdm(train_loader, desc=f\"[Epoch {epoch+1}] Training\"):\n",
    "        noisy_batch = noisy_batch.to(device)\n",
    "        clean_batch = clean_batch.to(device)\n",
    "\n",
    "        # === Generator forward pass ===\n",
    "        x_recon, vqvae_loss = model(noisy_batch, clean_batch)\n",
    "\n",
    "        # === Discriminator update ===\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        real_preds = discriminator(clean_batch)\n",
    "        fake_preds = discriminator(x_recon.detach())  # Don't backprop through G\n",
    "\n",
    "        real_loss = bce(real_preds, torch.ones_like(real_preds))\n",
    "        fake_loss = bce(fake_preds, torch.zeros_like(fake_preds))\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # === Generator update ===\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Forward pass again for grad tracking\n",
    "        x_recon, vqvae_loss = model(noisy_batch, clean_batch)\n",
    "        fake_preds = discriminator(x_recon)\n",
    "        adv_loss = bce(fake_preds, torch.ones_like(fake_preds))\n",
    "\n",
    "        g_loss = vqvae_loss + lambda_adv * adv_loss\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        train_loss += g_loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for noisy_batch in inference_loader:\n",
    "            noisy_batch = noisy_batch[0].to(device)\n",
    "\n",
    "            x_recon, loss = model(noisy_batch, noisy_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(inference_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # === Early stopping based on val loss ===\n",
    "    if val_loss < best_val_loss - 5e-5:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model (val loss: {val_loss:.6f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No val improvement. Patience {patience_counter}/{patience}\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f75762-9d1a-42d1-98fc-355fb2537a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0_no_lens = np.load(path + \"no_lens.npy\")\n",
    "\n",
    "# # Set up inference loader for all 20k\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# model.eval()\n",
    "# os.makedirs(\"combo_triplets_3\", exist_ok=True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, (img, label) in enumerate(tqdm(inference_loader, desc=\"Generating Triplets\")):\n",
    "#         img = img.to(device)\n",
    "#         recon, _ = model(img)\n",
    "#         heatmap = torch.abs(img - recon) * 3\n",
    "#         triplet = torch.cat([img, recon, heatmap], dim=1).squeeze().cpu().numpy()  # (3, 125, 125)\n",
    "#         lens_vis = x0_no_lens[i]\n",
    "\n",
    "#         save_dict = {\n",
    "#             \"triplet\": triplet,\n",
    "#             \"label\": label.item(),\n",
    "#             \"lens_vis\": lens_vis\n",
    "#         }\n",
    "\n",
    "#         np.savez_compressed(f\"combo_triplets_3/triplet_{i:05d}.npz\", **save_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5d5b483-e36e-40ef-98ff-9ba9a6624db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Triplets: 100%|██████████| 20000/20000 [08:18<00:00, 40.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up inference loader for all 20k\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "out_dir = \"combo_triplets_3_U_Net\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(inference_loader, desc=\"Generating Triplets\")):\n",
    "        # batch is (img, label, theta_e) OR (img, label, theta_e, arc)\n",
    "        img   = batch[0].to(device)          # (1,1,H,W)\n",
    "        label = int(batch[1].item())         # 0/1\n",
    "        theta = float(batch[2].item())       # scalar\n",
    "\n",
    "        recon, _ = model(img, img)           # (1,1,H,W)\n",
    "        heatmap = torch.abs(img - recon) * 3.0\n",
    "\n",
    "        triplet = torch.cat([img, recon, heatmap], dim=1) \\\n",
    "                      .squeeze(0).cpu().numpy().astype(np.float32)  # (3,H,W)\n",
    "\n",
    "        # OPTIONAL: store the actual input image as lens_vis for easy plotting later\n",
    "        lens_vis_np = img.squeeze(0).squeeze(0).detach().cpu().numpy().astype(np.float32)  # (H,W)\n",
    "\n",
    "        np.savez_compressed(\n",
    "            os.path.join(out_dir, f\"triplet_{i:05d}.npz\"),\n",
    "            triplet=triplet,\n",
    "            label=label,\n",
    "            theta_e=(theta if label == 1 else np.nan),  # theta only meaningful for lenses\n",
    "            lens_vis=lens_vis_np,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca2c98-0084-4c8f-87aa-f209ca61e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComboTripletDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None, return_lens_vis=True):\n",
    "        self.file_list = sorted([\n",
    "            os.path.join(folder_path, f)\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.endswith(\".npz\")\n",
    "        ])\n",
    "        self.transform = transform\n",
    "        self.return_lens_vis = return_lens_vis\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.file_list[idx])\n",
    "        combo = data[\"triplet\"]  # shape: (3, 125, 125)\n",
    "        label = int(data[\"label\"])\n",
    "\n",
    "        combo_tensor = torch.tensor(combo, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            combo_tensor = self.transform(combo_tensor)\n",
    "\n",
    "        if self.return_lens_vis:\n",
    "            lens_vis = torch.tensor(data[\"lens_vis\"], dtype=torch.float32).unsqueeze(0)  \n",
    "            return combo_tensor, label, lens_vis\n",
    "        else:\n",
    "            return combo_tensor, label\n",
    "\n",
    "class ComboTripletDatasetWithNoise(Dataset):\n",
    "    def __init__(self, folder_path, transform=None, return_lens_vis=True, add_noise=False, noise_std=0.1):\n",
    "        self.file_list = sorted([\n",
    "            os.path.join(folder_path, f)\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.endswith(\".npz\")\n",
    "        ])\n",
    "        self.transform = transform\n",
    "        self.return_lens_vis = return_lens_vis\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.file_list[idx])\n",
    "        combo = data[\"triplet\"]  \n",
    "        label = int(data[\"label\"])\n",
    "\n",
    "        combo_tensor = torch.tensor(combo, dtype=torch.float32)  # shape: (3, H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            combo_tensor = self.transform(combo_tensor)\n",
    "\n",
    "        # Add Gaussian noise to all 3 channels\n",
    "        if self.add_noise:\n",
    "            noise = torch.randn_like(combo_tensor) * self.noise_std\n",
    "            combo_tensor = combo_tensor + noise\n",
    "            combo_tensor = torch.clamp(combo_tensor, 0.0, 1.0)  # optional: clip to keep values valid\n",
    "\n",
    "        if self.return_lens_vis:\n",
    "            lens_vis = torch.tensor(data[\"lens_vis\"], dtype=torch.float32).unsqueeze(0)  # (1, 125, 125)\n",
    "            return combo_tensor, label, lens_vis\n",
    "        else:\n",
    "            return combo_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578ee3b-d48e-4a85-ae5f-dc586ed54c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "\n",
    "lr = 5e-5\n",
    "bs = 8\n",
    "patience = 40\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee56a8de-524a-4f17-97cc-315a73253f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: transforms.functional.resize(x, (380, 380))),\n",
    "])\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, get labels for stratified split\n",
    "temp_dataset = ComboTripletDataset(\"combo_triplets_3\", transform=transform)\n",
    "indices = list(range(len(temp_dataset)))\n",
    "labels = [int(np.load(temp_dataset.file_list[i])[\"label\"]) for i in indices]\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=labels)\n",
    "\n",
    "# Create dataset with noise for training\n",
    "train_dataset = ComboTripletDataset(\n",
    "    \"combo_triplets_3\",\n",
    "    transform=transform,\n",
    "    return_lens_vis=True,\n",
    ")\n",
    "# train_dataset.add_noise = True     # Inject noise flag\n",
    "# train_dataset.noise_std = 0.1      # Set noise std\n",
    "\n",
    "# Create clean dataset for validation\n",
    "val_dataset = ComboTripletDataset(\n",
    "    \"combo_triplets_3\",\n",
    "    transform=transform,\n",
    "    return_lens_vis=True,\n",
    ")\n",
    "# val_dataset.add_noise = False      # No noise on val\n",
    "\n",
    "# Wrap in Subset based on indices\n",
    "train_loader = DataLoader(Subset(train_dataset, train_idx), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(Subset(val_dataset, val_idx), batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b30bbf-298f-45ce-95b9-ca7ded9963d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# === Check dataset size split ===\n",
    "total = len(train_dataset)\n",
    "print(f\"Total samples: {total}\")\n",
    "print(f\"Train size: {len(train_idx)} ({len(train_idx)/total:.2%})\")\n",
    "print(f\"Val size:   {len(val_idx)} ({len(val_idx)/total:.2%})\")\n",
    "\n",
    "# === Check label balance ===\n",
    "train_labels = [int(np.load(train_dataset.file_list[i])[\"label\"]) for i in train_idx]\n",
    "val_labels = [int(np.load(val_dataset.file_list[i])[\"label\"]) for i in val_idx]\n",
    "\n",
    "print(\"\\nTrain Label Distribution:\", Counter(train_labels))\n",
    "print(\"Val Label Distribution:  \", Counter(val_labels))\n",
    "\n",
    "# === Check shape of a batch ===\n",
    "images, labels, _ = next(iter(train_loader))\n",
    "print(\"\\nSample batch shape:\", images.shape)  # Expect (8, 3, 380, 380)\n",
    "print(\"Sample labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93d01e-ff30-4fda-9b2d-a89e9c9af23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load EfficientNetV2-L\n",
    "efficientnet = models.efficientnet_v2_l(weights=\"EfficientNet_V2_L_Weights.DEFAULT\")\n",
    "\n",
    "# 3. Modify final classifier for binary classification\n",
    "num_features = efficientnet.classifier[1].in_features\n",
    "efficientnet.classifier[1] = nn.Linear(num_features, 2)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 4. Move model to device\n",
    "efficientnet = efficientnet.to(device)\n",
    "\n",
    "# 5. Loss + Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdaBelief(efficientnet.parameters(), lr=lr, eps=1e-8, betas=(0.9, 0.999), weight_decay=1e-5, rectify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd19b29-dc44-4ec4-b848-9b0b8238ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_recalls = []\n",
    "\n",
    "best_val_recall = 0.0\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "best_model_path = \"models/effnetv2l_best_2.pth\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # === Training ===\n",
    "    efficientnet.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for images, labels, _ in tqdm(train_loader, desc=f\"[Epoch {epoch+1}] Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = efficientnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    train_acc = correct / len(train_loader.dataset)\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # === Validation ===\n",
    "    efficientnet.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = efficientnet(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = correct / len(val_loader.dataset)\n",
    "    val_loss /= len(val_loader)\n",
    "    recall = recall_score(all_labels, all_preds, pos_label=1)\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_recalls.append(recall)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Recall (Lensed): {recall:.4f}\")\n",
    "    \n",
    "    # === Early stopping\n",
    "    save_model = False\n",
    "    \n",
    "    if recall > best_val_recall + 1e-4:\n",
    "        best_val_recall = recall\n",
    "        save_model = True\n",
    "        print(f\"New best recall: {recall:.4f}\")\n",
    "    \n",
    "    elif recall >= 1.0 and val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        save_model = True\n",
    "        print(f\"Perfect recall maintained; new best acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if save_model:\n",
    "        patience_counter = 0\n",
    "        torch.save(efficientnet.state_dict(), best_model_path)\n",
    "        print(\"Model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14059a1b-b59a-45b1-ae0b-52a62e56492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load best model\n",
    "efficientnet.load_state_dict(torch.load(\"models/effnetv2l_best_2.pth\"))\n",
    "efficientnet.eval()\n",
    "\n",
    "all_probs = []\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# === Inference ===\n",
    "with torch.no_grad():\n",
    "    for images, labels, _ in tqdm(val_loader):\n",
    "        images = images.to(device)\n",
    "        outputs = efficientnet(images)\n",
    "\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1]  # class 1 = lensed\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# === ROC AUC ===\n",
    "roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# === Classification Report ===\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "# === Plot Confusion Matrix ===\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=[\"Non-Lensed\", \"Lensed\"],\n",
    "            yticklabels=[\"Non-Lensed\", \"Lensed\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Threshold = 0.5)\")\n",
    "plt.show()\n",
    "\n",
    "# === Plot ROC Curve ===\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.8f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9c029-612c-4db1-93f1-827e8847d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load file paths\n",
    "triplet_dir = \"combo_triplets_3\"\n",
    "triplet_files = sorted([\n",
    "    os.path.join(triplet_dir, f) for f in os.listdir(triplet_dir) if f.endswith(\".npz\")\n",
    "])\n",
    "\n",
    "# Collect 10 of each class\n",
    "lensed_files = []\n",
    "non_lensed_files = []\n",
    "\n",
    "for f in triplet_files:\n",
    "    data = np.load(f)\n",
    "    label = int(data[\"label\"])\n",
    "    if label == 1 and len(lensed_files) < 10:\n",
    "        lensed_files.append(f)\n",
    "    elif label == 0 and len(non_lensed_files) < 10:\n",
    "        non_lensed_files.append(f)\n",
    "    if len(lensed_files) == 10 and len(non_lensed_files) == 10:\n",
    "        break\n",
    "\n",
    "# === OG visual triplet plot ===\n",
    "def plot_triplet(filepath, title_prefix=\"\"):\n",
    "    data = np.load(filepath)\n",
    "    triplet = data[\"triplet\"]\n",
    "    original, recon, heatmap = triplet\n",
    "    lens_vis = data[\"lens_vis\"] if \"lens_vis\" in data else None\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "    im0 = axs[0].imshow(original, cmap=\"gray\")\n",
    "    axs[0].set_title(f\"{title_prefix}Original\")\n",
    "    axs[0].axis(\"off\")\n",
    "    fig.colorbar(im0, ax=axs[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    im1 = axs[1].imshow(recon, cmap=\"gray\")\n",
    "    axs[1].set_title(\"Reconstruction\")\n",
    "    axs[1].axis(\"off\")\n",
    "    fig.colorbar(im1, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    im2 = axs[3].imshow(heatmap, cmap=\"hot\", vmin=0, vmax=30000)\n",
    "    axs[3].set_title(\"Heatmap\")\n",
    "    axs[3].axis(\"off\")\n",
    "    fig.colorbar(im2, ax=axs[3], fraction=0.046, pad=0.04)\n",
    "\n",
    "    if lens_vis is not None:\n",
    "        im3 = axs[2].imshow(lens_vis, cmap=\"gray\")\n",
    "        axs[2].set_title(\"No-Lens Ref\")\n",
    "    else:\n",
    "        im3 = axs[2].imshow(np.zeros_like(original), cmap=\"gray\")\n",
    "        axs[2].set_title(\"No Ref\")\n",
    "    axs[2].axis(\"off\")\n",
    "    fig.colorbar(im3, ax=axs[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "# === Display Lensed ===\n",
    "print(\"Showing 10 Lensed Examples\")\n",
    "for file in lensed_files:\n",
    "    plot_triplet(file, title_prefix=\"Lensed: \")\n",
    "    plt.show()\n",
    "\n",
    "# === Display Non-Lensed ===\n",
    "print(\"Showing 10 Non-Lensed Examples\")\n",
    "for file in non_lensed_files:\n",
    "    plot_triplet(file, title_prefix=\"Non-Lensed: \")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e3218f-b5ef-4067-a97e-741ecedf68c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# === Step 1: Load file paths ===\n",
    "triplet_dir = \"combo_triplets_3\"\n",
    "triplet_files = sorted([\n",
    "    os.path.join(triplet_dir, f) for f in os.listdir(triplet_dir) if f.endswith(\".npz\")\n",
    "])\n",
    "\n",
    "# Collect 10 of each class\n",
    "lensed_files = []\n",
    "non_lensed_files = []\n",
    "\n",
    "for f in triplet_files:\n",
    "    data = np.load(f)\n",
    "    label = int(data[\"label\"])\n",
    "    if label == 1 and len(lensed_files) < 10:\n",
    "        lensed_files.append(f)\n",
    "    elif label == 0 and len(non_lensed_files) < 10:\n",
    "        non_lensed_files.append(f)\n",
    "    if len(lensed_files) == 10 and len(non_lensed_files) == 10:\n",
    "        break\n",
    "\n",
    "# === Step 2: Define binning function ===\n",
    "def get_pixel_bins(recon, normalize=True):\n",
    "    pixels = recon.ravel()\n",
    "    total = pixels.size\n",
    "    bins = {}\n",
    "\n",
    "    # Create 20 bins: 0.00–0.05, 0.05–0.10, ..., 0.95–1.00\n",
    "    edges = np.arange(0.0, 1.01, 0.05)  # include 1.00\n",
    "\n",
    "    for i in range(len(edges) - 1):\n",
    "        lower = edges[i]\n",
    "        upper = edges[i + 1]\n",
    "        key = f\"{lower:.2f}–{upper:.2f}\"\n",
    "        # Include right edge only on the final bin\n",
    "        count = np.sum((pixels >= lower) & (pixels < upper)) if upper < 1.0 else np.sum((pixels >= lower) & (pixels <= upper))\n",
    "        bins[key] = count / total if normalize else count\n",
    "\n",
    "    return bins\n",
    "\n",
    "# === Step 3: Apply to each file ===\n",
    "def collect_stats(file_list):\n",
    "    stat_list = []\n",
    "    for f in file_list:\n",
    "        recon = np.load(f)[\"triplet\"][1]\n",
    "        stat_list.append(get_pixel_bins(recon))\n",
    "    return pd.DataFrame(stat_list)\n",
    "\n",
    "lensed_df = collect_stats(lensed_files)\n",
    "non_lensed_df = collect_stats(non_lensed_files)\n",
    "\n",
    "# === Step 4: Compute average bin percentages ===\n",
    "mean_lensed = lensed_df.mean()\n",
    "mean_non_lensed = non_lensed_df.mean()\n",
    "\n",
    "# === Step 5: Plot comparison ===\n",
    "x = np.arange(len(mean_lensed))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, mean_lensed, width, label='Lensed')\n",
    "plt.bar(x + width/2, mean_non_lensed, width, label='Non-Lensed')\n",
    "plt.xticks(x, mean_lensed.index, rotation=45)\n",
    "plt.ylabel(\"Fraction of pixels per image (normalized)\")\n",
    "plt.title(\"Pixel Value Distribution Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91d668-796e-4a34-8c86-50e8e9ecefb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# === Reuse your binning function ===\n",
    "def get_pixel_bins(recon, normalize=True):\n",
    "    pixels = recon.ravel()\n",
    "    total = pixels.size\n",
    "    bins = {}\n",
    "    edges = np.arange(0.0, 1.01, 0.05)\n",
    "\n",
    "    for i in range(len(edges) - 1):\n",
    "        lower = edges[i]\n",
    "        upper = edges[i + 1]\n",
    "        key = f\"{lower:.2f}–{upper:.2f}\"\n",
    "        count = np.sum((pixels >= lower) & (pixels < upper)) if upper < 1.0 else np.sum((pixels >= lower) & (pixels <= upper))\n",
    "        bins[key] = count / total if normalize else count\n",
    "\n",
    "    return bins\n",
    "\n",
    "# === Individual plots for lensed ===\n",
    "for i, f in enumerate(lensed_files):\n",
    "    recon = np.load(f)[\"triplet\"][1]\n",
    "    bins = get_pixel_bins(recon)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(bins.keys(), bins.values(), color='blue')\n",
    "    plt.title(f\"Lensed {i+1} — Pixel Value Distribution\")\n",
    "    plt.xlabel(\"Pixel Value Bin\")\n",
    "    plt.ylabel(\"Fraction of Pixels\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === Individual plots for non-lensed ===\n",
    "for i, f in enumerate(non_lensed_files):\n",
    "    recon = np.load(f)[\"triplet\"][1]\n",
    "    bins = get_pixel_bins(recon)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(bins.keys(), bins.values(), color='orange')\n",
    "    plt.title(f\"Non-Lensed {i+1} — Pixel Value Distribution\")\n",
    "    plt.xlabel(\"Pixel Value Bin\")\n",
    "    plt.ylabel(\"Fraction of Pixels\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4105f-12f3-4eba-8236-734c8560bc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab876f-3329-4e49-8b16-0ab65f11e02e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.6.0",
   "language": "python",
   "name": "pytorch-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
